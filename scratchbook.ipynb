{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "094e0453",
   "metadata": {},
   "source": [
    "## Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a16f802e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import yaml\n",
    "import numpy as np\n",
    "from itertools import zip_longest\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "def load_global(config=\"global\"):\n",
    "    with open(f\"configs/{config}.yaml\", \"r\") as f:\n",
    "        return yaml.safe_load(f)\n",
    "    \n",
    "cfg = load_global()\n",
    "cfm = load_global(config=\"datapp_de\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36baf272",
   "metadata": {},
   "source": [
    "## Target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a3d21dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = cfm[\"target_data_raw\"]\n",
    "df = pd.read_csv(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fc330e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mask: True if value is not NaN\n",
    "mask = df[\"DE11\"].notna()\n",
    "\n",
    "# Assign a group id: whenever a NaN occurs, the group number increases\n",
    "groups = mask.ne(mask.shift()).cumsum()\n",
    "\n",
    "# Filter only groups where values are numbers, then count their sizes\n",
    "lengths = df[mask].groupby(groups).size()\n",
    "\n",
    "# Calculate the average length\n",
    "average_length = lengths.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea642ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sequence length: 12.860486946318568\n"
     ]
    }
   ],
   "source": [
    "print(\"Average sequence length:\", average_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e16d6f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.10664560958363106)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"DED5\"].fillna(0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "079b23c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for midnight and day\n",
    "df_day = df[df['Date'].str.endswith('12:00:00')].copy()\n",
    "df_day['Date'] = df_day['Date'].str[:-9]\n",
    "\n",
    "# filter for 1980 to 2024\n",
    "df_day = df_day[(df_day['Date'] >= '1980-01-01') & (df_day['Date'] <= '2024-12-31')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b20e294",
   "metadata": {},
   "source": [
    "## MLWP Model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66634c12",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m ds_era5 = \u001b[43mxr\u001b[49m.open_zarr(\u001b[33m'\u001b[39m\u001b[33mgs://weatherbench2/datasets/era5/1959-2023_01_10-wb13-6h-1440x721_with_derived_variables.zarr\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      2\u001b[39m ds_pangu = xr.open_zarr(\u001b[33m'\u001b[39m\u001b[33mgs://weatherbench2/datasets/pangu/2018-2022_0012_0p25.zarr\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m ds_neuralgcm = xr.open_zarr(\u001b[33m'\u001b[39m\u001b[33mgs://weatherbench2/datasets/neuralgcm_deterministic/2020-512x256.zarr\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'xr' is not defined"
     ]
    }
   ],
   "source": [
    "ds_era5 = xr.open_zarr('gs://weatherbench2/datasets/era5/1959-2023_01_10-wb13-6h-1440x721_with_derived_variables.zarr')\n",
    "ds_pangu = xr.open_zarr('gs://weatherbench2/datasets/pangu/2018-2022_0012_0p25.zarr')\n",
    "ds_neuralgcm = xr.open_zarr('gs://weatherbench2/datasets/neuralgcm_deterministic/2020-512x256.zarr')\n",
    "ds_graphcast = xr.open_zarr('gs://weatherbench2/datasets/graphcast/2020/date_range_2019-11-16_2021-02-01_12_hours_derived.zarr')\n",
    "ds_era5_hour = xr.open_zarr('gs://weatherbench2/datasets/era5/1959-2023_01_10-full_37-1h-0p25deg-chunk-1.zarr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae9f618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Variables present in\n",
      "All datasets           All but neuralgcm     \n",
      "--------------------------------------------\n",
      "geopotential           10m_u_component_of_wind\n",
      "specific_humidity      10m_v_component_of_wind\n",
      "temperature            10m_wind_speed\n",
      "u_component_of_wind    2m_temperature\n",
      "v_component_of_wind    geopotential\n",
      "wind_speed             mean_sea_level_pressure\n",
      "                       specific_humidity\n",
      "                       temperature\n",
      "                       u_component_of_wind\n",
      "                       v_component_of_wind\n",
      "                       wind_speed\n"
     ]
    }
   ],
   "source": [
    "# Check which variables are in all datasets\n",
    "valid_variables1 = [v for v in ds_era5.data_vars if \n",
    "                   #v in ds_era5_hour.data_vars and\n",
    "                   v in ds_pangu.data_vars and \n",
    "                   v in ds_neuralgcm.data_vars and \n",
    "                   v in ds_graphcast.data_vars]\n",
    "\n",
    "valid_variables2 = [v for v in ds_pangu.data_vars if \n",
    "                   v in ds_era5.data_vars and \n",
    "                   #v in ds_era5_hour.data_vars and\n",
    "                   #v in ds_neuralgcm.data_vars and \n",
    "                   v in ds_graphcast.data_vars]\n",
    "print(\"          Variables present in\")\n",
    "print(f\"{\"All datasets\":<{22}} {\"All but neuralgcm\":<{22}}\")\n",
    "print(\"-\" * (22 + 22))\n",
    "for a, b in zip_longest(valid_variables1, valid_variables2, fillvalue=\"\"):\n",
    "    print(f\"{a:<22} {b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f27e663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10m_u_component_of_wind',\n",
       " '10m_v_component_of_wind',\n",
       " '2m_dewpoint_temperature',\n",
       " '2m_temperature',\n",
       " 'angle_of_sub_gridscale_orography',\n",
       " 'anisotropy_of_sub_gridscale_orography',\n",
       " 'boundary_layer_height',\n",
       " 'geopotential',\n",
       " 'geopotential_at_surface',\n",
       " 'high_vegetation_cover',\n",
       " 'lake_cover',\n",
       " 'land_sea_mask',\n",
       " 'leaf_area_index_high_vegetation',\n",
       " 'leaf_area_index_low_vegetation',\n",
       " 'low_vegetation_cover',\n",
       " 'mean_sea_level_pressure',\n",
       " 'mean_surface_latent_heat_flux',\n",
       " 'mean_surface_net_long_wave_radiation_flux',\n",
       " 'mean_surface_net_short_wave_radiation_flux',\n",
       " 'mean_surface_sensible_heat_flux',\n",
       " 'mean_top_downward_short_wave_radiation_flux',\n",
       " 'mean_top_net_long_wave_radiation_flux',\n",
       " 'mean_top_net_short_wave_radiation_flux',\n",
       " 'mean_vertically_integrated_moisture_divergence',\n",
       " 'potential_vorticity',\n",
       " 'sea_ice_cover',\n",
       " 'sea_surface_temperature',\n",
       " 'slope_of_sub_gridscale_orography',\n",
       " 'snow_depth',\n",
       " 'soil_type',\n",
       " 'specific_humidity',\n",
       " 'standard_deviation_of_filtered_subgrid_orography',\n",
       " 'standard_deviation_of_orography',\n",
       " 'surface_pressure',\n",
       " 'temperature',\n",
       " 'total_cloud_cover',\n",
       " 'total_column_water',\n",
       " 'total_column_water_vapour',\n",
       " 'total_precipitation',\n",
       " 'type_of_high_vegetation',\n",
       " 'type_of_low_vegetation',\n",
       " 'u_component_of_wind',\n",
       " 'v_component_of_wind',\n",
       " 'vertical_velocity',\n",
       " 'volumetric_soil_water_layer_1',\n",
       " 'volumetric_soil_water_layer_2',\n",
       " 'volumetric_soil_water_layer_3',\n",
       " 'volumetric_soil_water_layer_4']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v for v in ds_era5_hour.data_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82614d48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Time</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Lon</th>\n",
       "      <th>Lev</th>\n",
       "      <th>Lead Time</th>\n",
       "      <th>Time Res/h</th>\n",
       "      <th>Lead Time Res/h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ERA5</td>\n",
       "      <td>93544</td>\n",
       "      <td>721</td>\n",
       "      <td>1440</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[6]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ERA5 1h</td>\n",
       "      <td>561264</td>\n",
       "      <td>721</td>\n",
       "      <td>1440</td>\n",
       "      <td>37</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[1]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pangu</td>\n",
       "      <td>3652</td>\n",
       "      <td>721</td>\n",
       "      <td>1440</td>\n",
       "      <td>13</td>\n",
       "      <td>40.0</td>\n",
       "      <td>[12]</td>\n",
       "      <td>[6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NeuralGCM</td>\n",
       "      <td>797</td>\n",
       "      <td>256</td>\n",
       "      <td>512</td>\n",
       "      <td>37</td>\n",
       "      <td>31.0</td>\n",
       "      <td>[12]</td>\n",
       "      <td>[12]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GraphCast</td>\n",
       "      <td>886</td>\n",
       "      <td>721</td>\n",
       "      <td>1440</td>\n",
       "      <td>37</td>\n",
       "      <td>40.0</td>\n",
       "      <td>[12]</td>\n",
       "      <td>[6]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Dataset    Time  Lat   Lon  Lev  Lead Time Time Res/h Lead Time Res/h\n",
       "0       ERA5   93544  721  1440   13        NaN        [6]             NaN\n",
       "1    ERA5 1h  561264  721  1440   37        NaN        [1]             NaN\n",
       "2      Pangu    3652  721  1440   13       40.0       [12]             [6]\n",
       "3  NeuralGCM     797  256   512   37       31.0       [12]            [12]\n",
       "4  GraphCast     886  721  1440   37       40.0       [12]             [6]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check dimensions and resolution of datasets\n",
    "\n",
    "datasets = {\n",
    "    \"ERA5\": ds_era5,\n",
    "    \"ERA5 1h\": ds_era5_hour,\n",
    "    \"Pangu\": ds_pangu,\n",
    "    \"NeuralGCM\": ds_neuralgcm,\n",
    "    \"GraphCast\": ds_graphcast,\n",
    "}\n",
    "\n",
    "# Collect info about dimensions per dataset\n",
    "rows = []\n",
    "for name, ds in datasets.items():\n",
    "    dims = ds.sizes  # dict: {dim_name: size}\n",
    "    # Try to find matching dims; if missing -> set to None\n",
    "    time = dims.get(\"time\") or dims.get(\"times\") or None\n",
    "    lat = dims.get(\"lat\") or dims.get(\"latitude\") or None\n",
    "    lon = dims.get(\"lon\") or dims.get(\"longitude\") or None\n",
    "    lev = dims.get(\"level\") or dims.get(\"lev\") or dims.get(\"plev\") or None\n",
    "    lead = dims.get(\"prediction_timedelta\") or dims.get(\"lead\") or None\n",
    "\n",
    "    rows.append([name, time, lat, lon, lev, lead])\n",
    "\n",
    "# Create DataFrame\n",
    "df_dims = pd.DataFrame(rows, columns=[\"Dataset\", \"Time\", \"Lat\", \"Lon\", \"Lev\", \"Lead Time\"])\n",
    "\n",
    "# find time resolutions\n",
    "for name, ds in datasets.items():\n",
    "    time = ds.coords.get(\"time\")\n",
    "    if time is not None:\n",
    "        deltas = np.diff(time.values)  # differences between consecutive times\n",
    "        unique_deltas = np.unique(deltas)\n",
    "\n",
    "        #from ns to hours\n",
    "        unique_deltas = [int(delta / np.timedelta64(1, 'h')) for delta in unique_deltas]\n",
    "        df_dims.loc[df_dims[\"Dataset\"] == name, \"Time Res/h\"] = str(unique_deltas)\n",
    "\n",
    "    lead = ds.coords.get(\"prediction_timedelta\")\n",
    "    if lead is not None:\n",
    "        deltas = np.diff(lead.values)  # differences between consecutive lead times\n",
    "        unique_deltas = np.unique(deltas)\n",
    "\n",
    "        #from ns to hours\n",
    "        unique_deltas = [int(delta / np.timedelta64(1, 'h')) for delta in unique_deltas]\n",
    "        df_dims.loc[df_dims[\"Dataset\"] == name, \"Lead Time Res/h\"] = str(unique_deltas)\n",
    " \n",
    "\n",
    "df_dims\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0545cf0",
   "metadata": {},
   "source": [
    "## Training (ERA5) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a850d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_era5 = xr.open_zarr(cfm[\"era5_dataset_url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa801775",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ds_era5 = (\n",
    "    ds_era5\n",
    "    .sel(latitude=slice(cfm[\"feature_region\"][\"lat_max\"], cfm[\"feature_region\"][\"lat_min\"]),\n",
    "         longitude=slice(cfm[\"feature_region\"][\"lon_min\"], cfm[\"feature_region\"][\"lon_max\"]))\n",
    "    [cfm[\"feature_variables\"].keys()]\n",
    "    .sel(level=cfm[\"feature_level\"])\n",
    "    .sel(time=ds_era5['time'].dt.hour == 12)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb7dcdfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u_component_of_wind:   356.30 MB\n",
      "v_component_of_wind:   356.30 MB\n",
      "temperature    :   356.30 MB\n",
      "geopotential   :   356.30 MB\n",
      "specific_humidity:   356.30 MB\n",
      "Total size:  1781.50 MB\n"
     ]
    }
   ],
   "source": [
    "sizes_vars = {}\n",
    "for var in new_ds_era5.data_vars:\n",
    "    sizes_vars[var] = new_ds_era5[var].nbytes\n",
    "    print(f\"{var:15}: {sizes_vars[var]/1024**2:8.2f} MB\")\n",
    "\n",
    "print(f\"Total size: {sum(sizes_vars.values())/1024**2:8.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f56a00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[#                                       ] | 2% Completed | 360.67 ss\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ProgressBar():\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     era5_geopotential = \u001b[43mnew_ds_era5\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgeopotential\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jan Kraayvanger\\PhD\\FENCAST\\.venv\\Lib\\site-packages\\xarray\\core\\dataarray.py:1167\u001b[39m, in \u001b[36mDataArray.load\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m   1137\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\u001b[38;5;28mself\u001b[39m, **kwargs) -> Self:\n\u001b[32m   1138\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Trigger loading data into memory and return this dataarray.\u001b[39;00m\n\u001b[32m   1139\u001b[39m \n\u001b[32m   1140\u001b[39m \u001b[33;03m    Data will be computed and/or loaded from disk or a remote source.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1165\u001b[39m \u001b[33;03m    Variable.load\u001b[39;00m\n\u001b[32m   1166\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1167\u001b[39m     ds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_to_temp_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1168\u001b[39m     new = \u001b[38;5;28mself\u001b[39m._from_temp_dataset(ds)\n\u001b[32m   1169\u001b[39m     \u001b[38;5;28mself\u001b[39m._variable = new._variable\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jan Kraayvanger\\PhD\\FENCAST\\.venv\\Lib\\site-packages\\xarray\\core\\dataset.py:556\u001b[39m, in \u001b[36mDataset.load\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    553\u001b[39m chunkmanager = get_chunked_array_type(*chunked_data.values())\n\u001b[32m    555\u001b[39m \u001b[38;5;66;03m# evaluate all the chunked arrays simultaneously\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m evaluated_data: \u001b[38;5;28mtuple\u001b[39m[np.ndarray[Any, Any], ...] = \u001b[43mchunkmanager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43mchunked_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(chunked_data, evaluated_data, strict=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    561\u001b[39m     \u001b[38;5;28mself\u001b[39m.variables[k].data = data\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jan Kraayvanger\\PhD\\FENCAST\\.venv\\Lib\\site-packages\\xarray\\namedarray\\daskmanager.py:85\u001b[39m, in \u001b[36mDaskManager.compute\u001b[39m\u001b[34m(self, *data, **kwargs)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute\u001b[39m(\n\u001b[32m     81\u001b[39m     \u001b[38;5;28mself\u001b[39m, *data: Any, **kwargs: Any\n\u001b[32m     82\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[np.ndarray[Any, _DType_co], ...]:\n\u001b[32m     83\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdask\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marray\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compute\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jan Kraayvanger\\PhD\\FENCAST\\.venv\\Lib\\site-packages\\dask\\base.py:681\u001b[39m, in \u001b[36mcompute\u001b[39m\u001b[34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[39m\n\u001b[32m    678\u001b[39m     expr = expr.optimize()\n\u001b[32m    679\u001b[39m     keys = \u001b[38;5;28mlist\u001b[39m(flatten(expr.__dask_keys__()))\n\u001b[32m--> \u001b[39m\u001b[32m681\u001b[39m     results = \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m repack(results)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.2032.0_x64__qbz5n2kfra8p0\\Lib\\queue.py:210\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m remaining <= \u001b[32m0.0\u001b[39m:\n\u001b[32m    209\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnot_empty\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._qsize():\n\u001b[32m    212\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ShutDown\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.2032.0_x64__qbz5n2kfra8p0\\Lib\\threading.py:363\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    364\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    365\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "with ProgressBar():\n",
    "    era5_geopotential = new_ds_era5[\"geopotential\"].load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125b96a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_vars = [v for v in new_ds_era5.data_vars]\n",
    "\n",
    "era5_datasets = {}\n",
    "for v in era5_vars:\n",
    "    era5_datasets[v] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b552cc6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('data/processed')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Path(cfg[\"data_processed_dir\"])\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d284f4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_era = xr.open_dataset(path / \"era5_de_u_component_of_wind.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "29604ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_temperature = xr.open_dataset(path / \"era5_de_temperature.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5957e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d95b1f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     32\u001b[39m n = \u001b[32m10\u001b[39m \u001b[38;5;66;03m# Anzahl der Würfel\u001b[39;00m\n\u001b[32m     33\u001b[39m w = \u001b[32m10\u001b[39m  \u001b[38;5;66;03m# Anzahl der Seiten pro Würfel\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[43mdice_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mdice_distribution\u001b[39m\u001b[34m(n, w)\u001b[39m\n\u001b[32m      9\u001b[39m roll_combinations = itertools.product(faces, repeat=n)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Berechne die Summe der Würfelergebnisse für jede Kombination\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m sums = [\u001b[38;5;28msum\u001b[39m(roll) \u001b[38;5;28;01mfor\u001b[39;00m roll \u001b[38;5;129;01min\u001b[39;00m roll_combinations]\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Berechne die Häufigkeit jeder Summe\u001b[39;00m\n\u001b[32m     15\u001b[39m sum_counts = np.bincount(sums, minlength=n)  \u001b[38;5;66;03m# Mindestgröße ist n (minimale Summe) bis n*w (maximale Summe)\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Funktion, um die Häufigkeitsverteilung für n Würfel mit w Seiten zu berechnen\n",
    "def dice_distribution(n, w):\n",
    "    # Erzeuge alle möglichen Kombinationen von n Würfeln (mit Werten von 1 bis w)\n",
    "    faces = range(1, w+1)\n",
    "    roll_combinations = itertools.product(faces, repeat=n)\n",
    "\n",
    "    # Berechne die Summe der Würfelergebnisse für jede Kombination\n",
    "    sums = [sum(roll) for roll in roll_combinations]\n",
    "\n",
    "    # Berechne die Häufigkeit jeder Summe\n",
    "    sum_counts = np.bincount(sums, minlength=n)  # Mindestgröße ist n (minimale Summe) bis n*w (maximale Summe)\n",
    "\n",
    "    # Erstelle die x-Achse (die möglichen Summen)\n",
    "    possible_sums = range(n, n*w + 1)\n",
    "\n",
    "    # Plot der Häufigkeitsverteilung\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(possible_sums, sum_counts[n:], width=0.8, color='skyblue', edgecolor='black')\n",
    "    plt.title(f'Häufigkeitsverteilung von {n}W{w}', fontsize=16)\n",
    "    plt.xlabel('Summe der Würfelergebnisse', fontsize=12)\n",
    "    plt.ylabel('Häufigkeit', fontsize=12)\n",
    "    plt.xticks(np.arange(n, n*w + 1, step=3))\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "# Beispiel: 3 Würfel mit 20 Seiten\n",
    "n = 10 # Anzahl der Würfel\n",
    "w = 10  # Anzahl der Seiten pro Würfel\n",
    "dice_distribution(n, w)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
